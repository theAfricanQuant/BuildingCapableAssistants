{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "This notebook focuses on enhancing large language models (LLMs) to become more reliable and capable assistants by integrating tools, improving information accuracy, and expanding their functionality. Key topics include:\n",
    "\n",
    "1. **Tool Integration**: LLMs can be augmented with tools to access real-time data, perform specific tasks (e.g., web searches, database queries), and overcome limitations in domain-specific or up-to-date knowledge.\n",
    "   \n",
    "2. **Research Assistant Application**: An example application demonstrates how connecting external data sources improves LLM performance by grounding responses in factual, current information.\n",
    "\n",
    "3. **Information Extraction**: Techniques for extracting structured information from unstructured documents are explored, enabling LLMs to better process and analyze complex data.\n",
    "\n",
    "4. **Fact-Checking**: Methods to mitigate hallucinations and inaccuracies in LLM outputs are discussed, including automatic fact-checking against evidence to ensure correctness and reduce misinformation.\n",
    "\n",
    "5. **Reasoning Strategies**: Advanced reasoning approaches are applied to further enhance the capabilities of LLMs in solving complex problems.\n",
    "\n",
    "This section explores how tools can enhance the capabilities of large language models (LLMs) by addressing their limitations, particularly in areas like complex calculations or accessing external data. Below is a concise summary:\n",
    "\n",
    "### Key Points on Tool Use:\n",
    "1. **Tool Integration**:\n",
    "   - LLMs struggle with tasks requiring precise computations or logical reasoning, where specialized tools (e.g., calculators) excel.\n",
    "   - Combining LLMs with tools improves their problem-solving abilities and enables them to perform exact computations.\n",
    "\n",
    "2. **Tool Components**:\n",
    "   - **Name**: A unique identifier for the tool within an agent's toolkit.\n",
    "   - **Function**: The executable logic or action the tool performs.\n",
    "   - **Description**: A textual explanation that helps the LLM (agent) select the appropriate tool for a task.\n",
    "   - **Args Schema**: An optional Pydantic BaseModel schema that provides parameter validation and examples for inputs.\n",
    "   - **Return Direct Flag**: Indicates whether the tool's result should be returned directly to the user.\n",
    "\n",
    "3. **Example: WikipediaQueryRun Tool**:\n",
    "   - Demonstrates the use of a built-in LangChain tool, `WikipediaQueryRun`, which wraps around Wikipedia for querying general knowledge.\n",
    "   - Initialization involves configuring parameters like `top_k_results` and `doc_content_chars_max`.\n",
    "   - The tool's attributes (`name`, `description`, `args`, `return_direct`) provide clarity on its purpose and usage.\n",
    "   - It can be invoked with either a dictionary input or a simple string query.\n",
    "\n",
    "4. **Custom Tools**:\n",
    "   - While LangChain offers many pre-built tools, creating custom tools allows for tailored functionality and deeper understanding of the underlying mechanics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n",
      "A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "{'query': {'title': 'Query', 'type': 'string'}}\n",
      "False\n",
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of \n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Initialize the tool\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "# Inspect tool properties\n",
    "print(tool.name)  # 'wikipedia'\n",
    "print(tool.description)  # Describes the tool's utility and expected input\n",
    "print(tool.args)  # JSON schema for inputs\n",
    "print(tool.return_direct)  # False, indicating results are processed further\n",
    "\n",
    "# Run the tool\n",
    "result = tool.run(\"langchain\")  # Query Wikipedia for \"langchain\"\n",
    "print(result)  # Returns a summary of the LangChain page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining custom tools\n",
    "To define custom tools in LangChain, you can follow approaches such as these:\n",
    "\n",
    "@tool decorator\n",
    "Subclassing BaseTool\n",
    "StructuredTool dataclass\n",
    "\n",
    "This section introduces the concept of using Python's **decorator syntax** to define tools in a more concise and intuitive way. Below is a summary of how this works:\n",
    "\n",
    "### Key Points on Tool Decorators:\n",
    "1. **Decorator Syntax**:\n",
    "   - Tools can be defined using Python decorators, which simplify the process of creating tools.\n",
    "   - The `@tool` decorator is used to mark a function as a tool.\n",
    "\n",
    "2. **Default Behavior**:\n",
    "   - By default, the decorator uses the function's name as the tool's name.\n",
    "   - The function's docstring serves as the tool's description, making it mandatory to include a clear and descriptive docstring.\n",
    "\n",
    "3. **Advantages**:\n",
    "   - Simplifies the creation of tools by embedding the tool's logic directly within a function.\n",
    "   - Encourages clarity through the use of docstrings, ensuring that the tool's purpose is well-documented.\n",
    "\n",
    "### Example of Using the `@tool` Decorator:\n",
    "Hereâ€™s an example of how to define a custom tool using the decorator:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Look up things online.\"\"\"\n",
    "    return \"LangChain\"\n",
    "\n",
    "search(\"What's the best application framework for LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Wikipedia page for 'LangChain'\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches Wikipedia for the given query and returns a summary of the top result.\n",
    "    Useful for answering general knowledge questions about people, places, companies, historical events, etc.\n",
    "    \"\"\"\n",
    "    # Simulated Wikipedia search logic (replace with actual implementation)\n",
    "    return f\"Summary of Wikipedia page for '{query}'\"\n",
    "\n",
    "# Using the tool\n",
    "result = search_wikipedia(\"LangChain\")\n",
    "print(result)  # Output: Summary of Wikipedia page for 'LangChain'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can customize the tool name and JSON args by passing them into the tool decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"should be a search query\")\n",
    "@tool(\"search-tool\", args_schema=SearchInput, return_direct=True)\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Look up things online.\"\"\"\n",
    "    return \"LangChain\"\n",
    "\n",
    "search(\"What's the best application framework for LLMs?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclassing BaseTool \n",
    "\n",
    "This provides the most flexibility for defining tools, allowing customization of behavior, complex logic, asynchronous operations, and error handling. This approach involves creating a class that includes tool metadata (name, description), input schema definitions, and synchronous/asynchronous execution methods. Below is a concise summary:\n",
    "\n",
    "Key Points on Subclassing BaseTool:\n",
    "Customization : Offers full control over tool behavior, beyond simple function execution.\n",
    "Complex Logic : Enables implementation of advanced or multi-step processes.\n",
    "Asynchronous Support : Handles async operations, useful for tasks like API calls.\n",
    "Error Handling : Allows tailored error management and logging specific to the tool.\n",
    "Structure : Includes metadata (name, description), input schemas, and execution methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional, Type \n",
    "from langchain.tools import BaseTool \n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun, CallbackManagerForToolRun,\n",
    ")\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"should be a search query\")\n",
    "class CustomSearchTool(BaseTool):\n",
    "    name = \"custom_search\"\n",
    "    description = \"useful for when you need to answer questions about current events\"\n",
    "    args_schema: Type[BaseModel] = SearchInput\n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        return \"LangChain\"\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")\n",
    "search = CustomSearchTool()\n",
    "search(\"What's the most popular tool for writing LLM apps?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "We define a custom input schema (SearchInput)\n",
    "The toolâ€™s name and description are class attributes\n",
    "The _run method implements the synchronous tool execution\n",
    "The _arun method is a placeholder for asynchronous execution\n",
    "Both methods include optional callback managers for advanced control\n",
    "This approach requires more code but provides flexibility for complex tool implementations and integration with LangChainâ€™s broader ecosystem.\n",
    "\n",
    "### StructuredTool `dataclass`\n",
    "\n",
    "StructuredTool offers a convenient way to define tools for Langchain workflows. It provides a balance between inheriting from the base BaseTool class (more complex) and simply using a decorator (less functionality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import StructuredTool\n",
    "def search_function(query: str):\n",
    "    return \"LangChain\"\n",
    "search = StructuredTool.from_function(\n",
    "    func=search_function,\n",
    "    name=\"Search\",\n",
    "    description=\"useful for when you need to answer questions about current events\",\n",
    ")\n",
    "search(\"Which framework has hundreds of integrations to use with LLMs?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preceding code defines a simple function search_function that always returns \"LangChain\". Then, it creates a StructuredTool object named search by specifying the function, name, and description. Finally, it calls the run method on the search tool with a query string, demonstrating how to use the tool.\n",
    "\n",
    "StructuredTool allows you to define a custom input schema using a BaseModel subclass. This provides better type checking and documentation for your toolâ€™s inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000000\n"
     ]
    }
   ],
   "source": [
    "from pydantic.v1 import BaseModel, Field  # <-- Use Pydantic v1 API\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "class CalculatorInput(BaseModel):\n",
    "    a: int = Field(description=\"first number\")\n",
    "    b: int = Field(description=\"second number\")\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "calculator = StructuredTool.from_function(\n",
    "    func=multiply,\n",
    "    name=\"Calculator\",\n",
    "    description=\"Multiply two numbers\",\n",
    "    args_schema=CalculatorInput,  # Make sure this is correctly passed\n",
    "    return_direct=True,\n",
    ")\n",
    "\n",
    "result = calculator.run({\"a\": 1_000_000_000, \"b\": 2})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and Using a Structured Tool in LangChain  \n",
    "\n",
    "In the example above, we define a `CalculatorInput` class to specify the expected input formatâ€”two integers. This class ensures proper type validation using Pydantic. The `multiply` function then utilizes this schema to enforce type safety.  \n",
    "\n",
    "We also set `return_direct=True`, which allows the functionâ€™s output to be returned immediately without additional processing. Finally, we call the `run` method with a dictionary containing values for `a` and `b`, executing the multiplication operation.  \n",
    "\n",
    "While `StructuredTool` simplifies tool creation, handling potential errors during execution is essential. The next section will explore strategies for implementing error-handling mechanisms in LangChain tools.  \n",
    "\n",
    "### Why We Used `pydantic.v1`  \n",
    "\n",
    "LangChain was originally built with **Pydantic v1**, but recent versions of Pydantic introduced breaking changes in v2. Some parts of LangChain **still expect Pydantic v1-style schemas**, which led to validation errors when using `pydantic` v2 directly.  \n",
    "\n",
    "To maintain compatibility, we explicitly imported from `pydantic.v1`:  \n",
    "\n",
    "```python\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "```\n",
    "\n",
    "This ensures that LangChain functions correctly interpret the input schema and avoid validation errors when using `StructuredTool`.  \n",
    "\n",
    "As LangChain continues evolving, future versions may fully support Pydantic v2, eliminating the need for this workaround. Until then, using `pydantic.v1` remains the most reliable solution for structured tool definitions.\n",
    "\n",
    "### Error handling\n",
    "Additionally, LangChain provides a way to handle tool errors. When a tool encounters an error and the exception is not caught, the agent will stop executing. To allow the agent to continue execution, you can raise a ToolException and set handle_tool_error accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The search tool is not available.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import ToolException\n",
    "def search_tool(s: str):\n",
    "    raise ToolException(\"The search tool is not available.\")\n",
    "search = StructuredTool.from_function(\n",
    "    func=search_tool,\n",
    "    name=\"Search_tool\",\n",
    "    description=\"A bad tool\",\n",
    "    handle_tool_error=True,\n",
    ")\n",
    "search(\"Search the internet and compress everything into a paragraph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing this should give you an error: `'The search tool is not available.'`\n",
    "\n",
    "You can also define a custom way to handle the tool error by setting `handle_tool_error` to a function that takes a `ToolException` as a parameter and returns a str value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops! Something went wrong: The search tool is not available.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import ToolException, StructuredTool\n",
    "\n",
    "# Define a custom error handler\n",
    "def custom_error_handler(error: ToolException) -> str:\n",
    "    return f\"Oops! Something went wrong: {error}\"\n",
    "\n",
    "# Define a faulty tool\n",
    "def search_tool(s: str):\n",
    "    raise ToolException(\"The search tool is not available.\")\n",
    "\n",
    "# Create the tool with custom error handling\n",
    "search = StructuredTool.from_function(\n",
    "    func=search_tool,\n",
    "    name=\"Search_tool\",\n",
    "    description=\"A bad tool\",\n",
    "    handle_tool_error=custom_error_handler,  # Pass custom error handler\n",
    ")\n",
    "\n",
    "# Run the tool\n",
    "result = search.run(\"Search the internet and compress everything into a paragraph!\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Research Assistant with LangChain  \n",
    "\n",
    "We will learn how to create a powerful research assistant by integrating an LLM with external tools for information gathering and query handling. We hope to break down LangChain agents, equipping ourself with the essentials to build an intelligent, efficient assistant.\n",
    "\n",
    "Letâ€™s start by defining a function to create a Langchain agent with a few basic tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import (\n",
    "    AgentExecutor, load_tools, create_react_agent\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "def load_agent() -> AgentExecutor:\n",
    "    llm = ChatOpenAI(temperature=0, streaming=True)\n",
    "    tools = load_tools(\n",
    "        tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"],\n",
    "        llm=llm\n",
    "    )\n",
    "    return AgentExecutor(\n",
    "        agent=create_react_agent(llm=llm, tools=tools), tools=tools\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_agent()` function initializes a LangChain agent with a **ChatOpenAI** model, enabling **streaming responses** for a better user experience. It loads tools like **DuckDuckGo (privacy-focused search), arXiv (academic research), and Wikipedia (entity information)** to enhance the agentâ€™s capabilities. The agent is built using the **ReAct (Reasoning + Acting) architecture** for decision-making.\n",
    "\n",
    "The discussion also mentions **alternative tools** available in LangChain, such as **Wolfram Alpha (math and logic), Google/Bing search, Tavily Search API (optimized for LLMs), and Open-Meteo (weather data)**.\n",
    "\n",
    "Finally, the text suggests **deploying the agent as a Streamlit app** to create an interactive web interface, leveraging Streamlitâ€™s simplicity and ML workflow optimization.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Overall Purpose**\n",
    "\n",
    "The following code constructs an interactive chatbot application using the Streamlit library. The chatbot leverages the power of a large language model (LLM) combined with external tools like arXiv and Wikipedia to provide comprehensive and informative responses to user queries. The underlying framework for the chatbot's reasoning and action selection is the ReAct (Reasoning and Acting) framework.\n",
    "\n",
    "**Code Breakdown**\n",
    "\n",
    "1. **Initialization and Setup**\n",
    "\n",
    "   ```python\n",
    "   from config import set_environment  # Import a custom function (presumably for setting API keys and environment variables)\n",
    "\n",
    "   set_environment()  # Call the function to configure the environment\n",
    "\n",
    "   import streamlit as st  # Import the Streamlit library for building the web application\n",
    "\n",
    "   from langchain_community.callbacks.streamlit import StreamlitCallbackHandler  # Import a callback handler for integrating with Streamlit\n",
    "\n",
    "   from langchain.prompts import PromptTemplate  # Import PromptTemplate for structuring prompts\n",
    "   from langchain.agents import AgentExecutor, load_tools, create_react_agent  # Import components for creating and managing agents\n",
    "   from langchain_community.chat_models import ChatOpenAI  # Import the ChatOpenAI language model\n",
    "   ```\n",
    "\n",
    "   - These lines import the necessary libraries and modules.\n",
    "   - `set_environment()` is assumed to handle any required environment configuration, such as setting API keys for the language model and tools.\n",
    "\n",
    "2. **`load_agent()` Function**\n",
    "\n",
    "   ```python\n",
    "   def load_agent() -> AgentExecutor:\n",
    "       \"\"\"\n",
    "       This function initializes and loads a ReAct agent with specified tools and prompt template.\n",
    "\n",
    "       Returns:\n",
    "           AgentExecutor: An agent executor ready to handle user requests.\n",
    "       \"\"\"\n",
    "       llm = ChatOpenAI(temperature=0.2, streaming=True)  # Initialize the language model with a temperature of 0.2 and streaming enabled.\n",
    "       tools = load_tools(tool_names=[\"arxiv\", \"wikipedia\"], llm=llm)  # Load the arXiv and Wikipedia tools for the agent to use.\n",
    "\n",
    "       # Define the prompt template for the agent, instructing it on the format for its responses.\n",
    "       prompt = PromptTemplate(\n",
    "           input_variables=[\"input\", \"tools\", \"tool_names\", \"agent_scratchpad\"],\n",
    "           template=(\n",
    "               \"You are an assistant with access to the following tools: {tool_names}\\n\\n\"\n",
    "               \"{tools}\\n\\n\"\n",
    "               \"Use the following format *EXACTLY*:\\n\"\n",
    "               \"Question: the input question\\n\"\n",
    "               \"Thought: consider what to do\\n\"\n",
    "               \"Action: the action to take, should be one of {tool_names}\\n\"\n",
    "               \"Action Input: the input to the action\\n\"\n",
    "               \"Observation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\n\"\n",
    "               \"Thought: I now know the final answer\\n\"\n",
    "               \"Final Answer: the final answer to the original input question\\n\\n\"\n",
    "               \"Question: {input}\\n\"\n",
    "               \"{agent_scratchpad}\"\n",
    "           ),\n",
    "       )\n",
    "\n",
    "       # Create the ReAct agent and return an agent executor.\n",
    "       agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "       return AgentExecutor(agent=agent, tools=tools)\n",
    "   ```\n",
    "\n",
    "   - This function encapsulates the logic for creating and configuring the ReAct agent.\n",
    "   - `llm = ChatOpenAI(temperature=0.2, streaming=True)`: Initializes the `ChatOpenAI` language model.\n",
    "     - `temperature=0.2`: Controls the randomness of the model's output. A lower temperature makes the output more deterministic.\n",
    "     - `streaming=True`: Enables streaming of the model's output, allowing for a more interactive user experience.\n",
    "   - `tools = load_tools(tool_names=[\"arxiv\", \"wikipedia\"], llm=llm)`: Loads the arXiv and Wikipedia tools. These tools allow the agent to access and retrieve information from these sources.\n",
    "   - `prompt = PromptTemplate(...)`: Defines a `PromptTemplate` that structures the instructions and format for the agent's responses. The template emphasizes a clear ReAct format, where the agent explicitly states its thought process, actions, and observations.\n",
    "   - `agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)`: Creates the ReAct agent using the provided language model, tools, and prompt template.\n",
    "   - `return AgentExecutor(agent=agent, tools=tools)`: Returns an `AgentExecutor` that manages the execution of the agent and its interactions with the tools.\n",
    "\n",
    "3. **Streamlit Application Logic**\n",
    "\n",
    "   ```python\n",
    "   chain = load_agent()  # Initialize the agent executor\n",
    "\n",
    "   # Get user input from the Streamlit chat interface\n",
    "   if prompt := st.chat_input():\n",
    "       st.chat_message(\"user\").write(prompt)  # Display the user's message\n",
    "       with st.chat_message(\"assistant\"):  # Context for the assistant's response\n",
    "           st_callback = StreamlitCallbackHandler(st.container())  # Set up a callback handler for Streamlit\n",
    "           response = chain.invoke(\n",
    "               {\"input\": prompt}, callbacks=[st_callback], handle_parsing_errors=True  # Invoke the agent with the user's prompt, callbacks, and error handling\n",
    "           )\n",
    "           print(f\"Response: {response}\")  # Print the complete response from the agent\n",
    "           if \"agent_outcome\" in response and response[\"agent_outcome\"] is not None:  # Check if the response contains agent outcome information\n",
    "               print(f\"Agent Outcome: {response['agent_outcome']}\")  # Print the agent outcome details\n",
    "           st.write(response[\"output\"])  # Display the agent's output in the chat interface\n",
    "   ```\n",
    "\n",
    "   - `chain = load_agent()`: Initializes the agent executor by calling the `load_agent()` function.\n",
    "   - `if prompt := st.chat_input():`: Checks if the user has provided input in the Streamlit chat interface.\n",
    "   - `st.chat_message(\"user\").write(prompt)`: If there is user input, display it in the chat as a user message.\n",
    "   - `with st.chat_message(\"assistant\"):`: Sets the context for the assistant's response in the chat.\n",
    "   - `st_callback = StreamlitCallbackHandler(st.container())`: Creates a `StreamlitCallbackHandler` to integrate the agent's intermediate steps (thoughts, actions, observations) with the Streamlit application, allowing for a more interactive display of the agent's reasoning process.\n",
    "   - `response = chain.invoke(...)`: Invokes the agent (using the `AgentExecutor`) with the user's prompt and additional parameters:\n",
    "     - `{\"input\": prompt}`: Provides the user's input as a dictionary with the key \"input.\"\n",
    "     - `callbacks=[st_callback]`: Includes the Streamlit callback handler to display the agent's thinking process.\n",
    "     - `handle_parsing_errors=True`: Enables error handling to allow the agent to recover from potential parsing errors during its response generation.\n",
    "   - `print(f\"Response: {response}\")`: Prints the complete response dictionary for debugging and inspection.\n",
    "   - `if \"agent_outcome\" in response and response[\"agent_outcome\"] is not None:`: Checks if the response dictionary contains information about the agent's outcome and prints it if available.\n",
    "   - `st.write(response[\"output\"])`: Extracts the final answer or output generated by the agent and displays it in the Streamlit chat interface.\n",
    "\n",
    "**In Summary**\n",
    "\n",
    "This code sets up a Streamlit-based chatbot application that utilizes a ReAct agent powered by a large language model. The agent can access external tools like arXiv and Wikipedia to gather information and provide comprehensive responses. The use of a prompt template enforces a clear and structured response format, while the Streamlit callback handler provides insights into the agent's reasoning process. The application is designed to be interactive, allowing users to engage in a conversation with the agent and receive informative answers to their questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
